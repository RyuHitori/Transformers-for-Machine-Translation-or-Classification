{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1fd93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fbdec366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_en</th>\n",
       "      <th>en</th>\n",
       "      <th>id_vi</th>\n",
       "      <th>vi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1280</td>\n",
       "      <td>Today is June 18th and it is Muiriel's birthday!</td>\n",
       "      <td>5665</td>\n",
       "      <td>H√¥m nay l√† ng√†y 18 th√°ng s√°u, v√† c≈©ng l√† ng√†y ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1282</td>\n",
       "      <td>Muiriel is 20 now.</td>\n",
       "      <td>5667</td>\n",
       "      <td>B√¢y gi·ªù Muiriel ƒë∆∞·ª£c 20 tu·ªïi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1283</td>\n",
       "      <td>The password is \"Muiriel\".</td>\n",
       "      <td>5668</td>\n",
       "      <td>M·∫≠t m√£ l√† \"Muiriel\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1286</td>\n",
       "      <td>I'm at a loss for words.</td>\n",
       "      <td>5671</td>\n",
       "      <td>T√¥i h·∫øt l·ªùi ƒë·ªÉ n√≥i.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1286</td>\n",
       "      <td>I'm at a loss for words.</td>\n",
       "      <td>3481583</td>\n",
       "      <td>T√¥i kh√¥ng bi·∫øt n√≥i g√¨.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_en                                                en    id_vi  \\\n",
       "0   1280  Today is June 18th and it is Muiriel's birthday!     5665   \n",
       "1   1282                                Muiriel is 20 now.     5667   \n",
       "2   1283                        The password is \"Muiriel\".     5668   \n",
       "3   1286                          I'm at a loss for words.     5671   \n",
       "4   1286                          I'm at a loss for words.  3481583   \n",
       "\n",
       "                                                  vi  \n",
       "0  H√¥m nay l√† ng√†y 18 th√°ng s√°u, v√† c≈©ng l√† ng√†y ...  \n",
       "1                      B√¢y gi·ªù Muiriel ƒë∆∞·ª£c 20 tu·ªïi.  \n",
       "2                               M·∫≠t m√£ l√† \"Muiriel\".  \n",
       "3                                T√¥i h·∫øt l·ªùi ƒë·ªÉ n√≥i.  \n",
       "4                             T√¥i kh√¥ng bi·∫øt n√≥i g√¨.  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Sentence pairs in English-Vietnamese - 2025-11-12.tsv', sep='\\t')\n",
    "df.columns = [\"id_en\", \"en\", \"id_vi\", \"vi\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7856020f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(95)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['en_word_count'] = df['en'].str.split().str.len()\n",
    "df['en_word_count'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd4ee53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(122)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vi_word_count'] = df['vi'].str.split().str.len()\n",
    "df['vi_word_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81cbbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dff74946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "import regex as re\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "\n",
    "    # Replace special punctuation forms\n",
    "    s = s.replace(\"‚Ä¶\", \".\")\n",
    "    s = s.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"').replace(\"‚Äô\", \"'\")\n",
    "\n",
    "    # Add space around .?! to tokenize clearly\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1 \", s)\n",
    "\n",
    "    # Keep letters (any language), numbers, and .?!\n",
    "    s = re.sub(r\"[^\\p{L}\\p{N}.!?']+\", \" \", s)\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f330bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    filepath = \"Sentence pairs in English-Vietnamese - 2025-11-12.tsv\"\n",
    "\n",
    "    lines = open(filepath, encoding='utf-8').read().strip().split(\"\\n\")\n",
    "\n",
    "    pairs = []\n",
    "    for l in lines:\n",
    "        parts = l.split(\"\\t\")\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        eng = normalizeString(parts[1])\n",
    "        vie = normalizeString(parts[3])\n",
    "        pairs.append([eng, vie])\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "395c0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 123\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    # REMOVE filtering if you want full dataset\n",
    "    # pairs = filterPairs(pairs)\n",
    "    # print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bef777cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 18580 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 7493\n",
      "vie 3860\n",
      "['it goes without saying that tom is in love with kathy .', 'r√µ r√†ng l√† tom ƒëang y√™u kathy .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'vie', False)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4aba9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8cab46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3138b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1c30586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "# def get_dataloader(batch_size):\n",
    "#     input_lang, output_lang, pairs = prepareData('eng', 'vie', False)\n",
    "\n",
    "#     n = len(pairs)\n",
    "    \n",
    "#     input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "#     target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "#     for idx, (inp, tgt) in enumerate(pairs):\n",
    "#         inp_ids = indexesFromSentence(input_lang, inp)\n",
    "#         tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "#         inp_ids.append(EOS_token)\n",
    "#         tgt_ids.append(EOS_token)\n",
    "#         input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "#         target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "#     train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "#                                torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "#     train_sampler = RandomSampler(train_data)\n",
    "#     train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "#     return input_lang, output_lang, train_dataloader\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'vie', False)\n",
    "\n",
    "    n = len(pairs)\n",
    "\n",
    "    # allocate padded arrays\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        # convert to index list\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "\n",
    "        # üö® REQUIRED: truncate BEFORE adding EOS\n",
    "        inp_ids = inp_ids[:MAX_LENGTH - 1]\n",
    "        tgt_ids = tgt_ids[:MAX_LENGTH - 1]\n",
    "\n",
    "        # append EOS token\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "\n",
    "        # fill padded arrays (now guaranteed to fit)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    # create tensor dataset\n",
    "    train_data = TensorDataset(\n",
    "        torch.LongTensor(input_ids).to(device),\n",
    "        torch.LongTensor(target_ids).to(device)\n",
    "    )\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return input_lang, output_lang, pairs, train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3423a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80d7774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e332561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb83ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "03d874a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab5bffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 18580 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 7493\n",
      "vie 3860\n",
      "51m 26s (- 771m 40s) (5 6%) 0.3795\n",
      "101m 10s (- 708m 14s) (10 12%) 0.2362\n",
      "150m 32s (- 652m 20s) (15 18%) 0.1756\n",
      "199m 44s (- 599m 14s) (20 25%) 0.1409\n",
      "256m 52s (- 565m 7s) (25 31%) 0.1188\n",
      "307m 11s (- 511m 59s) (30 37%) 0.1029\n",
      "366m 35s (- 471m 20s) (35 43%) 0.0911\n",
      "421m 28s (- 421m 28s) (40 50%) 0.0817\n",
      "487m 53s (- 379m 28s) (45 56%) 0.0743\n",
      "529m 48s (- 317m 53s) (50 62%) 0.0682\n",
      "573m 13s (- 260m 33s) (55 68%) 0.0630\n",
      "630m 54s (- 210m 18s) (60 75%) 0.0586\n",
      "691m 19s (- 159m 32s) (65 81%) 0.0549\n",
      "738m 40s (- 105m 31s) (70 87%) 0.0516\n",
      "782m 48s (- 52m 11s) (75 93%) 0.0488\n",
      "838m 6s (- 0m 0s) (80 100%) 0.0463\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, pairs, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498675b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> she cried throughout the night\n",
      "= co ay a khoc suot em\n",
      "< ban a thay ban ban ban toi a thay ban <EOS>\n",
      "\n",
      "> he takes care of his appearance\n",
      "= anh ay cham chut ve be ngoai cua minh\n",
      "< anh ay la anh ay la anh ay cua anh ay thich cua anh ay <EOS>\n",
      "\n",
      "> i m good with people\n",
      "= toi co kha nang tuong tac xa hoi tot\n",
      "< ban va noi voi nguoi nghi khi ban va noi voi nguoi <EOS>\n",
      "\n",
      "> this book makes pleasant reading\n",
      "= quyen sach nay oc that thu vi\n",
      "< ong vat va muon nen lam viec noi cua ban <EOS>\n",
      "\n",
      "> what makes you think we won t succeed ?\n",
      "= ieu gi khien ban nghi rang chung ta se khong thanh cong ?\n",
      "< ban se nghi se se khong muon lam ieu o se khong ? <EOS>\n",
      "\n",
      "> yesterday i translated a video and composed subtitles in esperanto and spanish\n",
      "= hom qua toi a dich va viet phu e cho mot video bang tieng esperanto va tieng tay ban nha\n",
      "< mot nguoi va va se va va se va va va se va va va se va va va se mot nguoi phap va va se ngu <EOS>\n",
      "\n",
      "> he is not a bad person\n",
      "= anh ta khong phai la nguoi xau\n",
      "< mot nguoi muon song <EOS>\n",
      "\n",
      "> i want to buy a new smartphone\n",
      "= toi muon mua mot cai ien thoai moi\n",
      "< mot nguoi hoi mot nguoi phap va co mot nguoi phap <EOS>\n",
      "\n",
      "> we do need your advice\n",
      "= chung toi co can loi khuyen cua ban\n",
      "< ban nen lam viec cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban than cua ban\n",
      "\n",
      "> i didn t know that you had company\n",
      "= tui khong biet la cau co nguoi qua\n",
      "< tom se khong muon lam gi e noi la tom <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
