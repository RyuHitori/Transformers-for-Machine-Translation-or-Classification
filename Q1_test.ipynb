{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b5791d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import math, time, random, re, unicodedata\n",
    "from io import open\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4821702",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 125\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d1b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Text cleaning & tokenization (adapted from your original funcs)\n",
    "# -----------------------------\n",
    "import regex as re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = s.replace(\"…\", \".\")\n",
    "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^\\p{L}\\p{N}.!?']+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecbe0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Vocabulary class (with PAD,SOS,EOS,UNK)\n",
    "# -----------------------------\n",
    "PAD = '<PAD>'\n",
    "SOS = '<SOS>'\n",
    "EOS = '<EOS>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2idx = {PAD:0, SOS:1, EOS:2, UNK:3}\n",
    "        self.idx2word = {0:PAD, 1:SOS, 2:EOS, 3:UNK}\n",
    "        self.freq = {}\n",
    "        self.size = 4\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for w in sentence.split(' '):\n",
    "            self.add_word(w)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.size\n",
    "            self.idx2word[self.size] = word\n",
    "            self.freq[word] = 1\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.freq[word] += 1\n",
    "\n",
    "    def encode(self, sentence, max_len=MAX_LENGTH):\n",
    "        ids = [self.word2idx.get(w, self.word2idx[UNK]) for w in sentence.split(' ')]\n",
    "        ids = ids[:max_len-1]\n",
    "        ids.append(self.word2idx[EOS])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            if i == self.word2idx[EOS]:\n",
    "                words.append('<EOS>')\n",
    "                break\n",
    "            words.append(self.idx2word.get(int(i), UNK))\n",
    "        return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e115b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: Sentence pairs in English-Vietnamese - 2025-11-12.tsv\n",
      "Total pairs: 18580\n",
      "Input vocab size: 7495\n",
      "Output vocab size: 3862\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Read file & preprocess\n",
    "# -----------------------------\n",
    "FILEPATH = 'Sentence pairs in English-Vietnamese - 2025-11-12.tsv'\n",
    "print('Loading file:', FILEPATH)\n",
    "raw = open(FILEPATH, encoding='utf-8').read().strip().split('\\n')\n",
    "pairs = []\n",
    "for line in raw:\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    en = normalizeString(parts[1])\n",
    "    vi = normalizeString(parts[3])\n",
    "    pairs.append((en, vi))\n",
    "print('Total pairs:', len(pairs))\n",
    "\n",
    "# Build vocabs\n",
    "input_vocab = Vocab('eng')\n",
    "output_vocab = Vocab('vie')\n",
    "for en, vi in pairs:\n",
    "    input_vocab.add_sentence(en)\n",
    "    output_vocab.add_sentence(vi)\n",
    "print('Input vocab size:', input_vocab.size)\n",
    "print('Output vocab size:', output_vocab.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af619594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes - train: 14864 val: 1858 test: 1858\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Train/Val/Test split\n",
    "# -----------------------------\n",
    "N = len(pairs)\n",
    "indices = list(range(N))\n",
    "random.shuffle(indices)\n",
    "train_end = int(0.8 * N)\n",
    "val_end = int(0.9 * N)\n",
    "train_idx = indices[:train_end]\n",
    "val_idx = indices[train_end:val_end]\n",
    "test_idx = indices[val_end:]\n",
    "\n",
    "train_pairs = [pairs[i] for i in train_idx]\n",
    "val_pairs = [pairs[i] for i in val_idx]\n",
    "test_pairs = [pairs[i] for i in test_idx]\n",
    "print('Split sizes - train:', len(train_pairs), 'val:', len(val_pairs), 'test:', len(test_pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1753fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset & collate_fn (padding)\n",
    "# -----------------------------\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.src = src_vocab\n",
    "        self.tgt = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en, vi = self.pairs[idx]\n",
    "        src_ids = self.src.encode(en)\n",
    "        tgt_ids = [self.tgt.word2idx[SOS]] + self.tgt.encode(vi)  # decoder input includes SOS\n",
    "        return torch.LongTensor(src_ids), torch.LongTensor(tgt_ids)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_lens = [len(x) for x in src_batch]\n",
    "    tgt_lens = [len(x) for x in tgt_batch]\n",
    "    src_max = max(src_lens)\n",
    "    tgt_max = max(tgt_lens)\n",
    "    src_padded = torch.full((len(batch), src_max), input_vocab.word2idx[PAD], dtype=torch.long)\n",
    "    tgt_padded = torch.full((len(batch), tgt_max), output_vocab.word2idx[PAD], dtype=torch.long)\n",
    "    for i, (s, t) in enumerate(zip(src_batch, tgt_batch)):\n",
    "        src_padded[i, :len(s)] = s\n",
    "        tgt_padded[i, :len(t)] = t\n",
    "    return src_padded.to(device), tgt_padded.to(device)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = TranslationDataset(train_pairs, input_vocab, output_vocab)\n",
    "val_dataset = TranslationDataset(val_pairs, input_vocab, output_vocab)\n",
    "test_dataset = TranslationDataset(test_pairs, input_vocab, output_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44132f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Positional Encoding\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=MAX_LENGTH):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fcb6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Transformer Seq2Seq model\n",
    "# -----------------------------\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8,\n",
    "                 num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model, padding_idx=input_vocab.word2idx[PAD])\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model, padding_idx=output_vocab.word2idx[PAD])\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout, max_len=MAX_LENGTH)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=True)\n",
    "        self.generator = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def make_src_key_padding_mask(self, src):\n",
    "        # src: (batch, src_len)\n",
    "        return (src == input_vocab.word2idx[PAD])\n",
    "\n",
    "    def make_tgt_key_padding_mask(self, tgt):\n",
    "        return (tgt == output_vocab.word2idx[PAD])\n",
    "\n",
    "    def make_tgt_mask(self, tgt_len):\n",
    "        # causal mask (tgt_len, tgt_len)\n",
    "        mask = torch.triu(torch.ones((tgt_len, tgt_len), device=device) == 1, diagonal=1)\n",
    "        mask = mask.float().masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt_input):\n",
    "        # src: (batch, src_len)\n",
    "        # tgt_input: (batch, tgt_len) including SOS at position 0\n",
    "        src_emb = self.src_tok_emb(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        tgt_emb = self.tgt_tok_emb(tgt_input) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "\n",
    "        src_key_padding_mask = self.make_src_key_padding_mask(src)  # (batch, src_len)\n",
    "        tgt_key_padding_mask = self.make_tgt_key_padding_mask(tgt_input)  # (batch, tgt_len)\n",
    "        tgt_mask = self.make_tgt_mask(tgt_input.size(1))\n",
    "\n",
    "        out = self.transformer(src_emb, tgt_emb,\n",
    "                               tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_key_padding_mask,\n",
    "                               tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                               memory_key_padding_mask=src_key_padding_mask)\n",
    "        logits = self.generator(out)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47415115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LSTM baseline (encoder + attn decoder)\n",
    "# -----------------------------\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, n_layers=1, dropout=0.2):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=input_vocab.word2idx[PAD])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, n_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (batch, src_len)\n",
    "        emb = self.embedding(src)\n",
    "        outputs, (h, c) = self.lstm(emb)\n",
    "        # outputs: (batch, src_len, hidden*2)\n",
    "        return outputs, (h, c)\n",
    "\n",
    "class BahdanauAttn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttn, self).__init__()\n",
    "        self.W1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, enc_outputs, dec_hidden):\n",
    "        # enc_outputs: (batch, src_len, hidden*2)\n",
    "        # dec_hidden: (batch, hidden)\n",
    "        dec_hidden = dec_hidden.unsqueeze(1)  # (batch,1,hidden)\n",
    "        score = self.V(torch.tanh(self.W1(enc_outputs) + self.W2(dec_hidden)))  # (batch,src_len,1)\n",
    "        attn_weights = torch.softmax(score, dim=1)  # (batch,src_len,1)\n",
    "        context = torch.sum(attn_weights * enc_outputs, dim=1)  # (batch, hidden*2)\n",
    "        return context, attn_weights\n",
    "\n",
    "class DecoderLSTMAttn(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, enc_hidden_size, dec_hidden_size):\n",
    "        super(DecoderLSTMAttn, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=output_vocab.word2idx[PAD])\n",
    "        self.attn = BahdanauAttn(enc_hidden_size)\n",
    "        self.lstm = nn.LSTM(embed_size + enc_hidden_size*2, dec_hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, enc_outputs):\n",
    "        # input_step: (batch,1)\n",
    "        emb = self.embedding(input_step)  # (batch,1,embed)\n",
    "        # last_hidden is tuple (h,c) from LSTM decoder: use h[-1]\n",
    "        h = last_hidden[0][-1]  # (batch, hidden)\n",
    "        context, attn_weights = self.attn(enc_outputs, h)\n",
    "        context = context.unsqueeze(1)\n",
    "        lstm_input = torch.cat((emb, context), dim=2)\n",
    "        output, hidden = self.lstm(lstm_input, last_hidden)\n",
    "        output = output.squeeze(1)\n",
    "        output = self.out(output)\n",
    "        return output, hidden, attn_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af67b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training / inference utilities\n",
    "# -----------------------------\n",
    "def create_padding_mask(seq, pad_idx):\n",
    "    return (seq == pad_idx)\n",
    "\n",
    "# Greedy decoding for Transformer\n",
    "@torch.no_grad()\n",
    "def greedy_decode_transformer(model, src_sentence, src_vocab, tgt_vocab, max_len=MAX_LENGTH):\n",
    "    model.eval()\n",
    "    src_ids = src_vocab.encode(src_sentence)\n",
    "    src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(device)\n",
    "    # initial tgt input: SOS\n",
    "    tgt_ids = [tgt_vocab.word2idx[SOS]]\n",
    "    for i in range(max_len-1):\n",
    "        tgt_tensor = torch.LongTensor(tgt_ids).unsqueeze(0).to(device)\n",
    "        logits = model(src_tensor, tgt_tensor)  # (1, tgt_len, vocab)\n",
    "        next_token = logits[0, -1].argmax().item()\n",
    "        tgt_ids.append(next_token)\n",
    "        if next_token == tgt_vocab.word2idx[EOS]:\n",
    "            break\n",
    "    return tgt_ids\n",
    "\n",
    "# Greedy decoding for LSTM baseline (step-by-step)\n",
    "@torch.no_grad()\n",
    "def greedy_decode_lstm(encoder, decoder, src_sentence, src_vocab, tgt_vocab, max_len=MAX_LENGTH):\n",
    "    encoder.eval(); decoder.eval()\n",
    "    src_ids = src_vocab.encode(src_sentence)\n",
    "    src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(device)\n",
    "    enc_outputs, (h, c) = encoder(src_tensor)\n",
    "    # initialize decoder hidden - project encoder h to decoder size if needed\n",
    "    # For simplicity we'll initialize decoder hidden as zeros with proper shape\n",
    "    dec_h = torch.zeros(1, 1, decoder.lstm.hidden_size, device=device)\n",
    "    dec_c = torch.zeros(1, 1, decoder.lstm.hidden_size, device=device)\n",
    "    input_tok = torch.LongTensor([tgt_vocab.word2idx[SOS]]).unsqueeze(0).to(device)\n",
    "    preds = [tgt_vocab.word2idx[SOS]]\n",
    "    for _ in range(max_len-1):\n",
    "        out, (dec_h, dec_c), attn = decoder(input_tok, (dec_h, dec_c), enc_outputs)\n",
    "        next_tok = out.argmax(dim=1).item()\n",
    "        preds.append(next_tok)\n",
    "        if next_tok == tgt_vocab.word2idx[EOS]:\n",
    "            break\n",
    "        input_tok = torch.LongTensor([next_tok]).unsqueeze(0).to(device)\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b705822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU evaluation (corpus-level)\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def compute_bleu(model, loader, src_vocab, tgt_vocab, method='transformer'):\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    for src_batch, tgt_batch in loader:\n",
    "        # src_batch: (1, src_len) ; tgt_batch: (1, tgt_len)\n",
    "        src_sentence_ids = src_batch[0].cpu().tolist()\n",
    "        # decode reference (remove leading SOS if present)\n",
    "        ref_ids = tgt_batch[0].cpu().tolist()\n",
    "        # reference text tokens\n",
    "        # Remove SOS if present in ref_ids\n",
    "        if ref_ids and ref_ids[0] == tgt_vocab.word2idx[SOS]:\n",
    "            ref_ids = ref_ids[1:]\n",
    "        # truncate at EOS\n",
    "        if tgt_vocab.word2idx[EOS] in ref_ids:\n",
    "            ref_ids = ref_ids[:ref_ids.index(tgt_vocab.word2idx[EOS])]\n",
    "        ref_tokens = [tgt_vocab.idx2word[idx] for idx in ref_ids if idx != tgt_vocab.word2idx[PAD]]\n",
    "        references.append([ref_tokens])\n",
    "\n",
    "        src_sentence = ' '.join([src_vocab.idx2word[i] for i in src_sentence_ids if i != src_vocab.word2idx[PAD]])\n",
    "        if method == 'transformer':\n",
    "            pred_ids = greedy_decode_transformer(model, src_sentence, src_vocab, tgt_vocab)\n",
    "        else:\n",
    "            # method == 'lstm'\n",
    "            pred_ids = greedy_decode_lstm(model['enc'], model['dec'], src_sentence, src_vocab, tgt_vocab)\n",
    "        # postprocess preds: remove SOS and EOS\n",
    "        if pred_ids and pred_ids[0] == tgt_vocab.word2idx[SOS]:\n",
    "            pred_ids = pred_ids[1:]\n",
    "        if tgt_vocab.word2idx[EOS] in pred_ids:\n",
    "            pred_ids = pred_ids[:pred_ids.index(tgt_vocab.word2idx[EOS])]\n",
    "        hyp_tokens = [tgt_vocab.idx2word.get(i, UNK) for i in pred_ids]\n",
    "        hypotheses.append(hyp_tokens)\n",
    "\n",
    "    bleu = corpus_bleu(references, hypotheses)  # default weights for 4-gram\n",
    "    return bleu, references, hypotheses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a926e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training loops\n",
    "# -----------------------------\n",
    "def train_transformer(model, train_loader, val_loader, n_epochs=5, lr=1e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_vocab.word2idx[PAD])\n",
    "    model.to(device)\n",
    "    best_val = float('inf')\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for src_batch, tgt_batch in train_loader:\n",
    "            # tgt_batch includes SOS at position 0\n",
    "            tgt_input = tgt_batch[:, :-1]\n",
    "            tgt_out = tgt_batch[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(src_batch, tgt_input)  # (batch, tgt_len, vocab)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            tgt_out = tgt_out.contiguous().view(-1)\n",
    "            loss = criterion(logits, tgt_out)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch} Train Loss: {avg_train_loss:.4f}')\n",
    "        # validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for src_batch, tgt_batch in val_loader:\n",
    "                tgt_input = tgt_batch[:, :-1]\n",
    "                tgt_out = tgt_batch[:, 1:]\n",
    "                logits = model(src_batch, tgt_input)\n",
    "                logits = logits.view(-1, logits.size(-1))\n",
    "                tgt_out = tgt_out.contiguous().view(-1)\n",
    "                loss = criterion(logits, tgt_out)\n",
    "                val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f'  Val Loss: {avg_val_loss:.4f}')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e5d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM train (teacher forcing)\n",
    "def train_lstm(encoder, decoder, train_loader, val_loader, n_epochs=5, lr=1e-3):\n",
    "    encoder.to(device); decoder.to(device)\n",
    "    enc_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    dec_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_vocab.word2idx[PAD])\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        encoder.train(); decoder.train()\n",
    "        total_loss = 0.0\n",
    "        for src_batch, tgt_batch in train_loader:\n",
    "            # tgt_batch has SOS + target+EOS\n",
    "            # we will iterate time-step by time-step using teacher forcing\n",
    "            enc_outputs, (h, c) = encoder(src_batch)\n",
    "            batch_size, tgt_len = tgt_batch.size()\n",
    "            dec_input = tgt_batch[:, 0].unsqueeze(1)  # SOS\n",
    "            dec_hidden = (torch.zeros(1, batch_size, decoder.lstm.hidden_size, device=device),\n",
    "                          torch.zeros(1, batch_size, decoder.lstm.hidden_size, device=device))\n",
    "            enc_optimizer.zero_grad(); dec_optimizer.zero_grad()\n",
    "            loss = 0.0\n",
    "            for t in range(1, tgt_len):\n",
    "                out, dec_hidden, attn = decoder(dec_input, dec_hidden, enc_outputs)\n",
    "                # out: (batch, vocab)\n",
    "                target = tgt_batch[:, t]\n",
    "                loss += criterion(out, target)\n",
    "                # teacher forcing\n",
    "                dec_input = tgt_batch[:, t].unsqueeze(1)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1.0)\n",
    "            enc_optimizer.step(); dec_optimizer.step()\n",
    "            total_loss += loss.item() / (tgt_len - 1)\n",
    "        avg_train = total_loss / len(train_loader)\n",
    "        print(f'LSTM Epoch {epoch} Train Loss: {avg_train:.4f}')\n",
    "        # validation omitted for brevity; can be added similarly\n",
    "    return {'enc': encoder, 'dec': decoder}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a598c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Instantiate models\n",
    "# -----------------------------\n",
    "D_MODEL = 256\n",
    "transformer = TransformerSeq2Seq(input_vocab.size, output_vocab.size, d_model=D_MODEL, nhead=8,\n",
    "                                 num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=1024, dropout=0.1).to(device)\n",
    "\n",
    "EMBED = 128\n",
    "HIDDEN = 256\n",
    "encoder_lstm = EncoderLSTM(input_vocab.size, EMBED, HIDDEN).to(device)\n",
    "decoder_lstm = DecoderLSTMAttn(output_vocab.size, EMBED, HIDDEN, HIDDEN).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Transformer (few epochs) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\torch\\nn\\functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Train (short example; increase epochs for real training)\n",
    "# -----------------------------\n",
    "print('\\n=== Training Transformer (few epochs) ===')\n",
    "transformer = train_transformer(transformer, train_loader, val_loader, n_epochs=3, lr=1e-4)\n",
    "\n",
    "print('\\n=== Training LSTM baseline (few epochs) ===')\n",
    "lstm_models = train_lstm(encoder_lstm, decoder_lstm, train_loader, val_loader, n_epochs=3, lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e289f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Evaluate BLEU on test set\n",
    "# -----------------------------\n",
    "print('\\n=== Evaluating Transformer on test set ===')\n",
    "bleu_transformer, refs_t, hyps_t = compute_bleu(transformer, test_loader, input_vocab, output_vocab, method='transformer')\n",
    "print('Transformer BLEU:', bleu_transformer)\n",
    "\n",
    "print('\\n=== Evaluating LSTM baseline on test set ===')\n",
    "bleu_lstm, refs_l, hyps_l = compute_bleu(lstm_models, test_loader, input_vocab, output_vocab, method='lstm')\n",
    "print('LSTM BLEU:', bleu_lstm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b22691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Print small comparison\n",
    "# -----------------------------\n",
    "print('\\n=== Sample comparisons (first 5 test examples) ===')\n",
    "for i in range(5):\n",
    "    src_ids, tgt_ids = test_dataset[i]\n",
    "    src_text = ' '.join([input_vocab.idx2word[idx.item()] for idx in src_ids if idx.item() != input_vocab.word2idx[PAD]])\n",
    "    ref_ids = tgt_ids.tolist()\n",
    "    if ref_ids[0] == output_vocab.word2idx[SOS]:\n",
    "        ref_ids = ref_ids[1:]\n",
    "    if output_vocab.word2idx[EOS] in ref_ids:\n",
    "        ref_ids = ref_ids[:ref_ids.index(output_vocab.word2idx[EOS])]\n",
    "    ref_text = ' '.join([output_vocab.idx2word[idx] for idx in ref_ids])\n",
    "    trans_pred_ids = greedy_decode_transformer(transformer, src_text, input_vocab, output_vocab)\n",
    "    if trans_pred_ids and trans_pred_ids[0] == output_vocab.word2idx[SOS]:\n",
    "        trans_pred_ids = trans_pred_ids[1:]\n",
    "    if output_vocab.word2idx[EOS] in trans_pred_ids:\n",
    "        trans_pred_ids = trans_pred_ids[:trans_pred_ids.index(output_vocab.word2idx[EOS])]\n",
    "    trans_text = ' '.join([output_vocab.idx2word.get(i, UNK) for i in trans_pred_ids])\n",
    "\n",
    "    lstm_pred_ids = greedy_decode_lstm(lstm_models['enc'], lstm_models['dec'], src_text, input_vocab, output_vocab)\n",
    "    if lstm_pred_ids and lstm_pred_ids[0] == output_vocab.word2idx[SOS]:\n",
    "        lstm_pred_ids = lstm_pred_ids[1:]\n",
    "    if output_vocab.word2idx[EOS] in lstm_pred_ids:\n",
    "        lstm_pred_ids = lstm_pred_ids[:lstm_pred_ids.index(output_vocab.word2idx[EOS])]\n",
    "    lstm_text = ' '.join([output_vocab.idx2word.get(i, UNK) for i in lstm_pred_ids])\n",
    "\n",
    "    print('\\nSRC :', src_text)\n",
    "    print('REF :', ref_text)\n",
    "    print('TRF :', trans_text)\n",
    "    print('LSTM:', lstm_text)\n",
    "\n",
    "print('\\nDone. Increase n_epochs and tune hyperparameters for better results.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
