{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e2faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "699f18e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\", index_col=False)\n",
    "data[\"sentiment\"] = data[\"sentiment\"].map({\"negative\":0, \"positive\":1})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e59ec2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8514e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5aa9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"review\"], truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fcd5318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:22<00:00, 2262.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd08e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd904fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=predictions, references=labels)[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=predictions, references=labels)[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels)[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdcbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ff353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7908bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40000/40000 [05:14<00:00, 127.05 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [01:22<00:00, 120.49 examples/s]\n",
      "Map: 100%|██████████| 40000/40000 [00:02<00:00, 18004.20 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 17825.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"review\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(data_train.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(data_test.reset_index(drop=True))\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"review\"])\n",
    "eval_dataset = eval_dataset.remove_columns([\"review\"])\n",
    "\n",
    "def add_labels(example):\n",
    "    example[\"labels\"] = example[\"sentiment\"]\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(add_labels)\n",
    "eval_dataset = eval_dataset.map(add_labels)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"sentiment\"])\n",
    "eval_dataset = eval_dataset.remove_columns([\"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f642d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomin\\AppData\\Local\\Temp\\ipykernel_23592\\78129325.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "d:\\python\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  12/5000 03:18 < 27:31:18, 0.05 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m training_args = TrainingArguments(\n\u001b[32m      2\u001b[39m     learning_rate=\u001b[32m2e-5\u001b[39m,\n\u001b[32m      3\u001b[39m     per_device_train_batch_size=\u001b[32m16\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     push_to_hub=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m trainer = Trainer(\n\u001b[32m     14\u001b[39m     model=model,\n\u001b[32m     15\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:905\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    899\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    903\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    914\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    915\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:724\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    720\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    721\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    722\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:531\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    529\u001b[39m     all_hidden_states = all_hidden_states + (hidden_state,)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:484\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    481\u001b[39m sa_output = \u001b[38;5;28mself\u001b[39m.sa_layer_norm(sa_output + x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m ffn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    485\u001b[39m ffn_output: torch.Tensor = \u001b[38;5;28mself\u001b[39m.output_layer_norm(ffn_output + sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    487\u001b[39m output = (ffn_output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:418\u001b[39m, in \u001b[36mFFN.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:424\u001b[39m, in \u001b[36mFFN.ff_chunk\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    422\u001b[39m x = \u001b[38;5;28mself\u001b[39m.activation(x)\n\u001b[32m    423\u001b[39m x = \u001b[38;5;28mself\u001b[39m.lin2(x)\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python\\Lib\\site-packages\\torch\\nn\\functional.py:1418\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ad7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fee590ac7964b77ac29923669e6eb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSFORMER MODEL PERFORMANCE ===\n",
      "{'eval_loss': 0.29022371768951416, 'eval_accuracy': 0.896, 'eval_precision': 0.9023354564755839, 'eval_recall': 0.8799171842650103, 'eval_f1': 0.8909853249475892, 'eval_runtime': 128.4223, 'eval_samples_per_second': 7.787, 'eval_steps_per_second': 0.973, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "#EVALUATE MODEL ON TEST SET\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\n=== TRANSFORMER MODEL PERFORMANCE ===\")\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8b904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f97de87c6804b618a79cffd325a4d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GET PREDICTIONS\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d791dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CONFUSION MATRIX\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"NEGATIVE\", \"POSITIVE\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix (BERT)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20522b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASELINE MODEL (TF-IDF + Logistic Regression) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      4961\n",
      "           1       0.89      0.91      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BASELINE MODEL (TF-IDF + Logistic Regression)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Using the SAME train-test split as BERT\n",
    "baseline_train = data_train\n",
    "baseline_test = data_test\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(baseline_train[\"review\"])\n",
    "X_test = vectorizer.transform(baseline_test[\"review\"])\n",
    "\n",
    "y_train = baseline_train[\"sentiment\"]\n",
    "y_test = baseline_test[\"sentiment\"]\n",
    "\n",
    "baseline_model = LogisticRegression(max_iter=200)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "baseline_preds = baseline_model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== BASELINE MODEL (TF-IDF + Logistic Regression) ===\")\n",
    "print(classification_report(y_test, baseline_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
